import loggingimport timeimport warningsfrom src.base.openai import MyChatOpenAIfrom src.base.module import LLMModulefrom abc import ABC, abstractmethod, ABCMetafrom src.base.utils import log_reply_thinkfrom src.base.utils import async_log_reply_thinkfrom typing import Anyfrom src.config import GPT_MODEL, AZURE_OPENAI_API_KEYfrom src.base.utils import async_process_inference_resultfrom src.base.utils import process_inference_resultfrom src.base.utils import add_call_func_2_logfrom src.base.utils import add_call_async_func_2_logfrom src.base.utils import store_result_in_attributefrom src.base.utils import check_setupfrom src.base.utils import async_check_setupclass _DecorateAllMethods(type):    """A metaclass that decorates all methods of a subclass.    This metaclass iterates over all attributes of a class being created. If an attribute is a callable    (i.e., a method), it checks if the method is overriding a method from its base classes. If it is,    and if the method name matches specific criteria, it applies a decorator to the method. This approach    ensures that methods overriding abstract methods still retain the intended decorators from the superclass.    """    def __new__(mcs, name, bases, dct):        # Iterate over all attributes of the class        for attr, value in dct.items():            # Check if the attribute is a callable (i.e., likely a method)            if callable(value):                is_over_write = False                # Check if this method overrides a method from its base classes                for base in bases:                    if hasattr(base, attr) and getattr(base, attr) is not value:                        is_over_write = True                        break                # Apply specific decorators based on the method's name and whether it overrides a base class method                if attr == 'initialize_prompt':                    dct[attr] = store_result_in_attribute('prompt')(add_call_func_2_log(value))                if attr == 'process_inference_results':                    dct[attr] = add_call_func_2_log(value)        # Create the new class with the possibly modified dictionary of attributes/methods        return super().__new__(mcs, name, bases, dct)class _CombinedMeta(ABCMeta, _DecorateAllMethods):    """    In order to avoid conflicts caused by other classes inherited by the class that are    not of the same origin as the primitive class, the primitive class and abstract class    ABCMeta are merged to avoid conflicts.    """    passclass ThinkModule(ABC, metaclass=_CombinedMeta):    """    A specialized module for extracting useful information in a conversational application.    It allows specifying a large language model and temperature value for the current    dialogue module.    """    def __init__(self,                 model_name: str = None,                 temperature: float = None,                 openai_api_key: str = None,                 verbose: bool = None):        """        Initialize the Think module        Args:            model_name: the name of the think module            temperature: the temperature of the think module            openai_api_key: the key of the module            verbose: if print the details        """        if model_name is not None:            self.model_name = model_name        if temperature is not None:            self.temperature = temperature        if openai_api_key is not None:            self.openai_api_key = openai_api_key        if verbose is not None:            self.verbose = verbose        self.module_type = 'think'    @abstractmethod    def initialize_prompt(self) -> str:        """        Returns the prompt template required for executing this dialogue module. You can        include placeholders in the prompt template to indicate possible content at        those positions.        Example:            #Character            Your name is {name}, your age is {age}, and you need to generate a            self-introduction response.        Returns:            The prompt template as a string.        """        pass    @async_process_inference_result    @add_call_async_func_2_log    @async_check_setup    async def async_think(self, **kwargs) -> Any:        """        Asynchronously performs inference for the current dialogue module.        Args:            **kwargs: Keyword arguments for inference.        Returns:            The inference result.        """        if hasattr(self, 'is_disable') and getattr(self, 'is_disable'):            setattr(self, 'is_disable', False)            return None        map_variable_2_value = {}        for input_variable in getattr(self, 'input_variables'):            if input_variable not in kwargs:                raise ValueError(                    f"The required placeholder {input_variable} is not present in the input "                    f"parameters {','.join([k for k in kwargs.keys()])}")            map_variable_2_value[input_variable] = kwargs.get(input_variable)        times = 3        for i in range(1, times + 1):            try:                result = await getattr(self, 'chatbot').chain.arun(**map_variable_2_value)                return result            except Exception as e:                logging.warning(f"arun error {e}")                time.sleep(0.05)                if i < times:                    warnings.warn(f"the think module {getattr(self, 'module_name', '')} fails at the {i}th trying. "                                  f"Now trays the {i + 1}th time.")                else:                    warnings.warn(f"the think module {getattr(self, 'module_name', '')} fails at the {i}th trying. "                                  f"Now return the empty results.")        return ""    @process_inference_result    @add_call_func_2_log    @check_setup    def think(self, **kwargs) -> Any:        """        Performs regular inference for the current dialogue module.        Args:            **kwargs: Keyword arguments for inference.        Returns:            The inference result.        """        if hasattr(self, 'is_disable') and getattr(self, 'is_disable'):            setattr(self, 'is_disable', False)            return None        map_variable_2_value = {}        for input_variable in getattr(self, 'input_variables'):            if input_variable not in kwargs:                raise ValueError(                    f"The required placeholder {input_variable} is not present in "                    f"the input parameters {','.join([k for k in kwargs.keys()])}")            map_variable_2_value[input_variable] = kwargs.get(input_variable)        times = 3        for i in range(1, times + 1):            try:                result = getattr(self, 'chatbot').chain.run(**map_variable_2_value)                return result            except Exception as e:                logging.warning(f"run error {e}")                time.sleep(0.5)                if i < times:                    warnings.warn(f"the think module {getattr(self, 'module_name', '')} fails at the {i}th trying. "                                  f"Now trays the {i + 1}th time.")                else:                    warnings.warn(f"the think module {getattr(self, 'module_name', '')} fails at the {i}th trying. "                                  f"Now return the empty results.")        return ""    @abstractmethod    def process_inference_results(self, inference_results: Any) -> Any:        """        This method allows for post-processing of the inference results before returning them.         The results obtained by the central control module are the output of this method.        Args:            inference_results: The results from the inference to be processed.        Returns:            The processed results.        """        pass    def setup(self,              llm: MyChatOpenAI = None,              verbose: bool = False):        """        Prepares the module for execution by initializing prompts and setting up the chatbot.        This function initializes necessary components for a module to function properly. It ensures        that the prompt is initialized, and a language model (LLM) is set up for interaction. If the        LLM is not provided as an argument, it creates a new instance using default or specified        parameters. Finally, it configures the module for replies and extracts input variables from        the chatbot's logic chain.        Args:            verbose: if print the details            llm (MyChatOpenAI, optional): An instance of MyChatOpenAI to be used by the module. If None,                                          a new instance is created using the module's configuration.                                          Defaults to None.        Note:            - `self.initialize_prompt()` should be a method that initializes or sets up the prompt              for the module, preparing it for interaction.            - `MyChatOpenAI` is assumed to be a class that encapsulates interaction with an OpenAI              model, where `model_name`, `temperature`, and `openai_api_key` are configuration parameters.            - `LLMModule.from_llm()` is assumed to be a factory method that creates an LLMModule              instance from a given LLM object, additional configurations like prompt and verbosity              level are also passed.            - This function modifies the instance (`self`) by setting up the chatbot, input variables,              and the module type directly.        """        # Initialize or set up the prompt for the module.        self.initialize_prompt()        setattr(self, 'verbose', getattr(self, 'verbose', verbose))        # If no LLM instance is provided, create a new one with specified configurations.        if llm is None:            llm = MyChatOpenAI(model_name=getattr(self, 'model_name', GPT_MODEL),                               temperature=getattr(self, 'temperature', 0),                               openai_api_key=getattr(self, 'openai_api_key', AZURE_OPENAI_API_KEY))        # Set up the chatbot using the provided or newly created LLM instance.        setattr(self, 'chatbot', LLMModule.from_llm(llm=llm,                                                    prompt=getattr(self, 'prompt'),                                                    verbose=self.verbose))        # Extract input variables from the chatbot's logic chain for further use.        setattr(self, 'input_variables', getattr(self, 'chatbot').chain.get_input_variables())