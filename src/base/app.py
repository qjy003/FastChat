import inspectimport asynciofrom fastapi import FastAPIfrom abc import ABC, abstractmethod, ABCMetafrom typing import Listfrom typing import Anyfrom typing import Union, Tuple, Dict, Callablefrom src.base.think import ThinkModulefrom src.base.reply import ReplyModulefrom src.base.front import FrontInterfacefrom src.base.query import QueryModulefrom src.config import PORT, AZURE_OPENAI_API_KEY, GPT_MODELfrom src.base.utils import first_call_initializefrom src.base.utils import check_return_in_chat_modulesfrom src.base.utils import store_result_in_attributefrom src.base.utils import then_call_process_think_resultsfrom src.base.utils import convert_async_2_normalfrom src.base.utils import then_call_process_inference_resultsfrom src.base.utils import print_info_after_runfrom src.base.utils import check_variable_typefrom src.base.utils import then_call_funcfrom src.base.utils import get_dict_valuefrom src.base.utils import return_type_must_befrom src.base.store import Storefrom src.base.evaluate import EvaluateModulefrom src.base.utils import then_app_call_evaluate_modulefrom src.base.utils import get_dict_valuesfrom src.base.utils import return_type_must_be_callablefrom src.base.http import HttpInterfacefrom src.base.utils import add_call_func_2_logfrom src.base.utils import add_call_async_func_2_logfrom src.base.openai import MyChatOpenAIfrom src.base.service import LogHttpServiceclass DecorateAllMethods(type):    """A metaclass that decorates all methods of a subclass.    This metaclass iterates over all attributes of a class being created. If an attribute is a callable    (i.e., a method), it checks if the method is overriding a method from its base classes. If it is,    and if the method name matches specific criteria, it applies a decorator to the method. This approach    ensures that methods overriding abstract methods still retain the intended decorators from the superclass.    """    def __new__(mcs, name, bases, dct):        # Iterate over all attributes of the class        for attr, value in dct.items():            # Check if the attribute is a callable (i.e., likely a method)            if callable(value):                is_over_write = False                # Check if this method overrides a method from its base classes                for base in bases:                    if hasattr(base, attr) and getattr(base, attr) is not value:                        is_over_write = True                        break                # Apply specific decorators based on the method's name and whether it overrides a base class method                if attr == 'update_chat_stage':                    dct[attr] = store_result_in_attribute('current_stage')(                        check_return_in_chat_modules(add_call_func_2_log(value)))                if attr == 'think' and is_over_write:                    dct[attr] = convert_async_2_normal(then_call_process_think_results(                        add_call_async_func_2_log(value)))                if attr == 'inference' and is_over_write:                    dct[attr] = add_call_func_2_log(then_call_process_inference_results(add_call_func_2_log(value)))                if attr == 'reply' and is_over_write:                    dct[attr] = convert_async_2_normal(value)                if attr == 'run' and is_over_write:                    dct[attr] = print_info_after_run(first_call_initialize(                        add_call_func_2_log(value)))                if attr == 'update_store':                    dct[attr] = store_result_in_attribute('store')(                        return_type_must_be(Store)(add_call_func_2_log(value)))                if attr == 'update_original_chat_stage':                    dct[attr] = store_result_in_attribute('current_stage')(store_result_in_attribute('original_stage')(                        check_return_in_chat_modules(add_call_func_2_log(value))))                if attr == 'gen_reply_inputs':                    dct[attr] = return_type_must_be(Dict)(add_call_func_2_log(value))                if attr == 'gen_evaluate_inputs':                    dct[attr] = then_app_call_evaluate_module(add_call_func_2_log(value))                if attr == 'gen_think_and_query_inputs':                    dct[attr] = return_type_must_be(Dict)(value)                if attr == 'request_preprocess':                    dct[attr] = then_call_func(return_type_must_be_callable(add_call_func_2_log(value)))                if attr == 'initialize':                    dct[attr] = add_call_func_2_log(value)                if attr == 'set_store_class':                    dct[attr] = store_result_in_attribute('store_class')(                        return_type_must_be(Store)(add_call_func_2_log(value)))                if attr == 'load_store':                    dct[attr] = store_result_in_attribute('store')(                        return_type_must_be(Store)(add_call_func_2_log(value)))        # Create the new class with the possibly modified dictionary of attributes/methods        return super().__new__(mcs, name, bases, dct)class CombinedMeta(ABCMeta, DecorateAllMethods):    """    In order to avoid conflicts caused by other classes inherited by the class that are    not of the same origin as the primitive class, the primitive class and abstract class    ABCMeta are merged to avoid conflicts.    """    passclass ChatApplication(ABC, metaclass=CombinedMeta):    """    A modular chat application framework designed to facilitate the development of conversational applications.    This class abstracts the complexities involved in implementing dialogue functionalities, reasoning capabilities,    and logging mechanisms. It automatically records the outcomes and prompts of each module, eliminating the need    for additional logic to handle these aspects. Furthermore, it integrates Gradio for frontend chat interfaces,    FastAPI for backend HTTP services, and connections to knowledge bases. The lifecycle of the chat application    is well-defined, requiring developers to implement the necessary logic within the provided abstract methods,    thereby reducing the chances of errors. In summary, this class serves as a development framework that adheres    to industry standards for quickly building conversational applications. Future efforts will focus on enhancing    low-code development capabilities and integrating fine-tuning with open-source large models for more efficient    complex application development.    Attributes:        application_name (str): The name of the chat application.        port (int): The port on which the chat application will be deployed.        openai_api_key (str): The API key for accessing large language models.        model_name (str): The large language model used by the application, which can be specified individually for each        module.        temperature (float): The temperature parameter for the large model, influencing its response variability.    """    def __init__(self,                 application_name: str = 'dialog app',                 port: int = PORT,                 openai_api_key: str = AZURE_OPENAI_API_KEY,                 model_name: str = GPT_MODEL,                 temperature: float = 0,                 verbose: bool = False):        """        Initializes an instance of the ChatApplication class.        Args:            application_name (str): The name of the chat application.            port (int): The port on which the chat application will be deployed.            openai_api_key (str): The API key for accessing large language models.            model_name (str): The large language model used by the application. Each module can specify its own model.            temperature (float): The temperature parameter for the large model, affecting the randomness of responses.        Initializes the chat application with the specified parameters, setting up the necessary infrastructure        for its operation, including HTTP interfaces, front-end interfaces, and module management.        """        self.port = port        self.store = None        self.app = None  # Placeholder for the main application instance (e.g., Gradio app)        self.http_app = None  # Placeholder for the FastAPI app instance        self.front_app = None  # Placeholder for frontend application interface        self.current_stage = None  # Tracks the current stage of the conversation        self.reply_modules = {}  # Stores reply modules with their configurations        self.think_modules = {}  # Stores think modules with their configurations        self.http_interfaces = {}  # Stores HTTP interface configurations        self.front_interfaces = {}  # Stores frontend interface configurations        self.key_values = {}  # Generic storage for key-value pairs        self.gpt_model = model_name  # The GPT model name for the application        self.temperature = temperature  # Temperature setting for the GPT model        self.openai_api_key = openai_api_key  # API keys for accessing the GPT model        self.application_name = application_name  # Name of the chat application        self.paths = []        self.query_modules = {}        self.evaluate_modules = {}        self.verbose = verbose        self.module_type = 'app'        self.module_name = application_name    def add_reply_module(self, reply_module: ReplyModule, module_name: str = None, is_initial_stage: bool = False):        """        Adds a reply module to the chat application, responsible for generating response content.        This method registers a reply module that will be used to handle replies within the chat application.        It also allows for setting a module as the initial stage of the conversation.        Args:            reply_module (ReplyModule): The reply module instance to be added.            module_name (str, optional): An identifier or name for the module. If not provided, it defaults to the file            name where the module class is defined.            is_initial_stage (bool, optional): A flag indicating whether this module should be considered as the initial             stage of the conversation.        Raises:            ValueError: If `reply_module` is not an instance of `ReplyModule`.            ValueError: If a module with the same `module_name` already exists in the application.        The method performs type checking on the `reply_module` to ensure it's an instance of `ReplyModule`.        If `module_name` is not specified, it uses the file name of the reply module's class as a default identifier.        It checks for name collisions in the existing modules and raises a `ValueError` if a duplicate is found.        If the `is_initial_stage` flag is set or if there are no other reply modules registered, it sets the        `current_stage` to this module's name.        """        if not isinstance(reply_module, ReplyModule):            raise ValueError(                f"the type of reply_module must be {ReplyModule.__name__}, not {type(reply_module).__name__}")        if module_name is None:            module_name = inspect.getfile(reply_module.__class__)        if module_name in self.reply_modules:            raise ValueError(f"{self.application_name} already has a reply module named {module_name}")        self.reply_modules[module_name] = reply_module        setattr(reply_module, 'application_name', self.application_name)        setattr(reply_module, 'module_name', module_name)        if is_initial_stage:            self.current_stage = module_name        elif len(self.reply_modules) <= 1:            self.current_stage = module_name    def add_think_module(self,                         think_module: ThinkModule,                         module_name: str = None,                         white_list: list[str] = None,                         black_list: list[str] = None):        """        Adds a think module to the chat application, responsible for analyzing and processing conversation content.        This method integrates a think module into the chat application. Think modules are designed to perform        reasoning and analysis on the dialogue content, enabling more sophisticated responses and interactions.        Each think module can be uniquely identified within the application, facilitating modular development        and easy management.        Args:            black_list: Whitelist list, the module will only be executed when it is in the dialogue stage in the                        whitelist            white_list: Blacklist list, when in the dialogue stage in the blacklist, the module will not execute            think_module (ThinkModule): The think module instance to be added.            module_name (str, optional): An identifier or unique name for the module. If not provided, it defaults to the            file name where the module class is defined.        Raises:            ValueError: If `think_module` is not an instance of `ThinkModule`.            ValueError: If a module with the same `module_name` already exists in the application.        The method checks if the provided `think_module` is indeed an instance of `ThinkModule`. If `module_name` is not        specified,        the method uses the file name of the think module's class as a default identifier. It ensures that there are no        duplicates by checking if a module with the same `module_name` already exists, raising a `ValueError` if so.        Upon successful validation,the think module is registered within the application under        the specified `module_name`.        Blacklist and whitelist cannot be set at the same time. Only blacklist or whitelist can be set, or neither can        be set.        """        if white_list is not None and black_list is not None:            raise ValueError("Blacklist and whitelist cannot be set at the same time.")        if not isinstance(think_module, ThinkModule):            raise ValueError(                f"the type of think_module must be {ThinkModule.__name__}, not {type(think_module).__name__}")        if module_name is None:            module_name = inspect.getfile(think_module.__class__)        if module_name in self.think_modules:            raise ValueError(f"the module_name {module_name} is duplicated in think_modules. Please change one.")        if module_name in self.query_modules:            raise ValueError(f"the module_name {module_name} is duplicated in query_modules. Please change one.")        self.think_modules[module_name] = think_module        setattr(think_module, 'application_name', self.application_name)        setattr(think_module, 'module_name', module_name)        if white_list is not None:            setattr(think_module, 'white_list', white_list)        if black_list is not None:            setattr(think_module, 'black_list', black_list)    def add_evaluate_module(self,                            evaluate_module: EvaluateModule,                            module_name: str = None,                            white_list: list[str] = None,                            black_list: list[str] = None):        """        Adds an evaluation module to the system. This method is strict and checks the type of the module being added,        as well as ensuring that the module name is not already in use within the evaluation modules.        Blacklist and whitelist cannot be set at the same time. Only blacklist or whitelist can be set, or neither can        be set.        Args:            black_list: Blacklist list, when in the dialogue stage in the blacklist, the module will not execute            white_list: Whitelist list, the module will only be executed when it is in the dialogue stage in the                        whitelist            evaluate_module: The evaluation module to be added.                An instance of EvaluateModule.            module_name: Optional; The name of the module. If not provided, the name will be inferred from the class                file of the evaluate_module.        Returns:            None.        Raises:            ValueError: If a module with the same name already exists in the evaluation modules.        """        if white_list is not None and black_list is not None:            raise ValueError('Blacklist and whitelist cannot be set at the same time.')        # Check if the evaluate_module is an instance of EvaluateModule        check_variable_type(evaluate_module, EvaluateModule)        # If module_name is not provided, use the file name of the class as the default name        if module_name is None:            module_name = inspect.getfile(evaluate_module.__class__)        # Check if module_name is a string        check_variable_type(module_name, str)        # Check for duplicate module names        if module_name in self.evaluate_modules:            raise ValueError(f"{self.application_name} already has an evaluate module named {module_name}")        # Add the evaluate_module to the dictionary with the module_name as the key        self.evaluate_modules[module_name] = evaluate_module        # Set the application_name and module_name attributes on the evaluate_module        setattr(evaluate_module, 'application_name', self.application_name)        setattr(evaluate_module, 'module_name', module_name)        if white_list is not None:            setattr(evaluate_module, 'white_list', white_list)        if black_list is not None:            setattr(evaluate_module, 'black_list', black_list)    @abstractmethod    def update_chat_stage(self,                          store: Store,                          think_results: Any,                          query_results: Any,                          original_chat_stage: str,                          inference_inputs: Any):        """        Updates the current conversation stage based on the results from the think module.        This is an abstract method that developers must implement in subclasses of the chat application.        The method should determine and return the identifier of the conversation stage to switch to,        based on the analysis and processing done by the think module.        Args:            original_chat_stage: the chat stage before update            inference_inputs: the request variables of the method inference            store: The store            query_results: The results returned by querying the knowledge database.            think_results: The results obtained from the execution of the think module.        Returns:            str: The identifier of the conversation stage that the chat application should transition to.        This method must be overridden by the subclass and is expected to use the results provided by the        think module, along with any additional arguments, to determine the next stage of the conversation.        The returned value must be a string that uniquely identifies the desired conversation stage.        """        pass    @convert_async_2_normal    @add_call_async_func_2_log    async def think(self,                    think_module_names: List[str] = None,                    is_return_async_tasks: bool = False, **kwargs) -> Union[Dict, list]:        """        Executes the thinking and reasoning process within the chat application.        This asynchronous method orchestrates the execution of specified think modules and        is intended to be used in conjunction with the `process_think_results` method,        which will handle the logic analysis of the results. The method leverages decorators        to convert the asynchronous call to a synchronous one and to ensure the results are        processed appropriately after the thinking process is complete.        Args:            is_return_async_tasks: is return the async tasks but not running            think_module_names (List[str], optional): A list of think module names that are to be                                                      executed during the thinking process. If not                                                      provided, all registered think modules will                                                      be executed.            **kwargs: Arbitrary keyword arguments that will be passed to each think module.        Returns:            dict: A dictionary mapping each think module name to its corresponding results.        Raises:            ValueError: If a think module name provided in `think_module_names` is not registered                        within the chat application.        This method initializes the execution of the think modules either specified by the        `think_module_names` parameter or all registered modules if `think_module_names` is None.        It then awaits the results of all asynchronous think operations, combines them into a        single dictionary, and returns this dictionary for further processing.        """        if think_module_names is None:            think_module_names = [model_name for model_name in self.think_modules.keys()]        think_module_names = [module_name for module_name in think_module_names if                              self.check_module_available(self.think_modules[module_name])]        for think_module_name in think_module_names:            if think_module_name not in self.think_modules:                raise ValueError(f"Think module {think_module_name} is not registered in the chat application")        think_modules = [self.think_modules.get(model_name) for model_name in think_module_names]        async_task = [think_module.async_think(**kwargs) for think_module in think_modules]        if is_return_async_tasks:            return async_task        results = await asyncio.gather(*async_task)        format_results = {model_name: results[i] for i, model_name in enumerate(think_module_names)}        return format_results    @convert_async_2_normal    @add_call_async_func_2_log    async def _think(self, is_return_async_tasks=True, **think_module_inputs) -> Union[dict, Tuple[list, list]]:        """        Executes the thinking and reasoning process within the chat application.        This asynchronous method orchestrates the execution of specified think modules and        is intended to be used in conjunction with the `process_think_results` method,        which will handle the logic analysis of the results. The method leverages decorators        to convert the asynchronous call to a synchronous one and to ensure the results are        processed appropriately after the thinking process is complete.        Args:            is_return_async_tasks: is return the async tasks but not running            think_module_names (List[str], optional): A list of think module names that are to be                                                      executed during the thinking process. If not                                                      provided, all registered think modules will                                                      be executed.            **kwargs: Arbitrary keyword arguments that will be passed to each think module.        Returns:            dict: A dictionary mapping each think module name to its corresponding results.        Raises:            ValueError: If a think module name provided in `think_module_names` is not registered                        within the chat application.        This method initializes the execution of the think modules either specified by the        `think_module_names` parameter or all registered modules if `think_module_names` is None.        It then awaits the results of all asynchronous think operations, combines them into a        single dictionary, and returns this dictionary for further processing.        """        think_module_names = [model_name for model_name in self.think_modules.keys() if                              self.check_module_available(self.think_modules[model_name])]        think_modules = [self.think_modules.get(model_name) for model_name in think_module_names]        async_task = [think_module.async_think(**think_module_inputs.get(think_module_names[i], think_module_inputs))                      for                      i, think_module in enumerate(think_modules)]        if is_return_async_tasks:            return think_module_names, async_task        results = await asyncio.gather(*async_task)        format_results = {model_name: results[i] for i, model_name in enumerate(think_module_names)}        return format_results    # Decorator to indicate that after the inference process, further processing will be applied to the results.    def inference(self, *args, **kwargs) -> Any:        """        Executes the core inference process of the chat application.        This method is responsible for handling requests from both the frontend chat interface and HTTP API requests.        It performs the main reasoning and response generation of the application. The return type is flexible (Any)        to accommodate different data formats and structures as agreed upon by backend developers, HTTP API, and        frontend interface teams.        Args:            *args: Variable length argument list for flexible argument passing.            **kwargs: Arbitrary keyword arguments for passing additional information to the inference process.        Returns:            Any: The result of the inference process, which may include the response for the frontend or HTTP API,                 as well as the original think results. The format and data type of the return value should be                 agreed upon by the development team.        This method orchestrates the thinking process by calling `think` with provided keyword arguments and then        generates a reply based on the think results. The decorator `@then_call_process_inference_results` suggests        that additional processing will be applied to the inference results before they are returned.        """        # Raise an error if any positional arguments are passed.        if len(args) > 0:            raise ValueError("Positional parameters cannot be passed when calling the inference method!")        # Initialize the Store class object.        self.store = self.set_store_class()()        # load the store        self.load_store(inference_inputs=kwargs, store=self.store)        # Generate inputs required for the thinking and querying modules.        think_and_query_inputs = self.gen_think_and_query_inputs(store=self.store, inference_inputs=kwargs)        # Update the current original chat stage with the inference results.        self.update_original_chat_stage(inference_inputs=kwargs)        # Perform the thinking and knowledge querying.        think_results, query_results = self._think_and_query(**think_and_query_inputs)        # Update the global store object with the new results.        self.update_store(store=self.store,                          think_results=think_results,                          query_results=query_results,                          original_chat_stage=getattr(self, 'original_stage'),                          inference_inputs=kwargs)        # Update the chat stage based on the new store and results.        self.update_chat_stage(store=self.store,                               think_results=think_results,                               query_results=query_results,                               original_chat_stage=getattr(self, 'original_stage'),                               inference_inputs=kwargs)        # Prepare the inputs for the chat module.        chat_module_inputs = self.gen_reply_inputs(current_chat_stage=self.current_stage,                                                   store=self.store,                                                   think_results=think_results,                                                   query_results=query_results,                                                   original_chat_stage=getattr(self, 'original_stage'),                                                   inference_inputs=kwargs)        # Execute the reply generation of the chat module.        reply_results = self.reply(**chat_module_inputs)        # Evaluate the quality of the generated reply.        evaluate_results = self.gen_evaluate_inputs(reply_results=reply_results,                                                    current_chat_stage=self.current_stage,                                                    store=self.store,                                                    think_results=think_results,                                                    query_results=query_results,                                                    original_chat_stage=getattr(self, 'original_stage'),                                                    inference_inputs=kwargs)        # Process the inference results and generate the final output.        final_results = self.process_inference_results(evaluate_results=evaluate_results,                                                       reply_results=reply_results,                                                       current_chat_stage=self.current_stage,                                                       store=self.store,                                                       think_results=think_results,                                                       query_results=query_results,                                                       original_chat_stage=getattr(self, 'original_stage'),                                                       inference_inputs=kwargs)        # Return the final results of the inference process.        return final_results    @abstractmethod    def gen_evaluate_inputs(self,                            reply_results: Any,                            current_chat_stage: str,                            store: Store,                            think_results: Dict,                            query_results: Dict,                            original_chat_stage: str,                            inference_inputs: Dict                            ) -> Any:        """        Evaluates the safety and appropriateness level of the large model's inference output.        This method is designed to assess the generated responses before they are sent back to the user,        categorizing them into levels based on their safety and appropriateness. It is recommended to use        a grading system where 0 represents 'dangerous', 1 represents 'concerning', and 2 represents 'safe'.        Args:            inference_inputs: the input parameters of the inference function            original_chat_stage: Tge chat stage before update            current_chat_stage: The chat stage after update            query_results: The results returned by the QueryModules            store: The store machine            reply_results: The results from the current dialogue module of the chat application.            think_results: The results from the thinking and reasoning module(s).        Returns:            Union[str, int]: The safety and appropriateness level of the reply. It is recommended to return                             an integer (0, 1, or 2) to represent the levels 'dangerous', 'concerning', and 'safe',                             respectively. However, the method is designed to allow flexibility in the return                             type, which could also be a string or any other type agreed upon by the development team.        Note:            This is an abstract method, meaning it must be implemented by any subclass inheriting from the class            that contains this method. The implementation should define the specific logic for evaluating the            safety and appropriateness of the reply results based on the provided inputs.        """        pass    @convert_async_2_normal    async def reply(self, **chat_module_inputs) -> str:        """        Generates a reply using the dialogue response module corresponding to the current stage.        This method delegates the task of generating a response to the appropriate reply module        based on the current stage of the conversation. The `@then_call_gen_evaluate_inputs` decorator        suggests that after generating the reply, the results will be passed to an evaluation function        to assess their safety and appropriateness.        Args:            **chat_module_inputs: Arbitrary keyword arguments that are passed to the reply module to generate a            response. These could include context, user input, and other relevant data needed for the reply.        Returns:            str: The generated response from the current dialogue module.        Raises:            ValueError: If the current dialogue stage is not present in the application's reply modules.        Note:            The method assumes that `self.current_stage` is an attribute representing the current stage of the            dialogue and that `self.reply_modules` is a dictionary where keys are stages and values are the            associated reply modules. The method will raise a ValueError if the current stage is not a key in            `self.reply_modules`.        """        if self.current_stage not in self.reply_modules:            raise ValueError(                f"Current dialogue stage {self.current_stage} is not present in the reply modules of the application")        result = await self.reply_modules[self.current_stage].async_reply(            **chat_module_inputs.get(self.current_stage, chat_module_inputs))        return result    from abc import abstractmethod    @abstractmethod    def process_inference_results(self,                                  evaluate_results: Dict,                                  reply_results: Any,                                  current_chat_stage: str,                                  store: Store,                                  think_results: Dict,                                  query_results: Dict,                                  original_chat_stage: str,                                  inference_inputs: Dict) -> Any:        """        Processes the results from the inference method of the chat application, enabling secondary manipulation        to meet various business requirements for the format of the returned results.        This abstract method is designed to be overridden by subclasses to implement specific logic for processing        and transforming the raw results obtained from the chat application's inference engine. This could involve        formatting the results, filtering based on certain criteria, or integrating additional information before        the results are returned to the user or another system component.        Args:            original_chat_stage: The chat stage before update            current_chat_stage: The current stage of the conversation            inference_inputs: parameters received by the inference method            query_results: results of knowledge query            store: global storage            evaluate_results: evaluation results            reply_results: The raw results from the chat application's reply generation component. This could include                           the generated text response, metadata, or other related information.            think_results: The raw results from the chat application's thinking or reasoning component. This may involve                           analysis, context understanding, or any preprocessing step's outcomes.        Returns:            The method's return type and structure are intentionally left unspecified, as they depend on the specific            implementation in subclasses. Implementers should define the return type based on the needs of their            application and the nature of the processing being performed.        Note:            As an abstract method, `process_inference_results` must be implemented by any subclass that inherits from            the class containing this method. The implementation details, including how the results are processed and            what is returned, are left to the discretion of the subclass.        """        pass    @abstractmethod    def initialize(self):        """        Invoked before the chat application service starts, for setting up dialogue modules, HTTP interfaces,        front-end interface connections, and thinking/reasoning modules.        This abstract method should be implemented in subclasses to include all necessary initializations required        for the chat application to function properly. It is designed to be a setup hook where developers can add or        configure various components such as dialogue handling modules, interfaces for communication with front-end        applications, HTTP endpoints for external API calls, and modules responsible for processing and reasoning        based on the input received.        Implementations might involve registering modules with the application, setting up routes for HTTP endpoints,        configuring connections to front-end interfaces, or initializing any algorithms or data structures needed for        thinking and reasoning processes.        Returns:            None: This method is expected to perform initializations and configurations, and does not return any value.        Note:            As an abstract method, `initialize` must be implemented by any subclass that inherits from the class            containing this method. The specifics of what needs to be initialized or configured can vary greatly            depending on the application's architecture and requirements, thus the implementation details are left            to the discretion of the subclass.        """        pass    @print_info_after_run    @first_call_initialize    @add_call_func_2_log    def run(self):        """        Starts the chat application service by first initializing all dialogue modules, thinking modules,        HTTP interfaces, and front-end interface connections, followed by launching the HTTP server.        This method activates the initialization process for all necessary components of the chat application        and then starts an HTTP server to handle incoming requests. The initialization is handled by the        `first_call_initialize` decorator, ensuring that all components are set up before the service becomes        available. The `print_info_after_run` decorator is assumed to output relevant information after the        service has started, such as service status or diagnostic messages.        The HTTP server is started using the Uvicorn ASGI server with the FastAPI application instance that        has been configured with the necessary routes and interface setups.        Returns:            None: This method is responsible for starting the service and does not return any value.        Raises:            Exception: If an error occurs during the initialization or server startup, it may raise an exception.                       The specific type of exception will depend on the nature of the failure and should be                       documented by the implementations of the initialization and server startup procedures.        """        llms = {}        all_modules = {}        all_modules.update(self.reply_modules)        all_modules.update(self.think_modules)        all_modules.update(self.evaluate_modules)        for module_name, module in all_modules.items():            llm_key = (getattr(module, 'model_name', self.gpt_model),                       getattr(module, 'temperature', self.temperature),                       getattr(module, 'openai_api_key', self.openai_api_key))            if llm_key not in llms:                llms[llm_key] = MyChatOpenAI(model_name=getattr(module, 'model_name', self.gpt_model),                                             temperature=getattr(module, 'temperature', self.temperature),                                             openai_api_key=getattr(module, 'openai_api_key', self.openai_api_key),                                             )            llm = llms[llm_key] if llm_key in llms else None            module.setup(llm=llm, verbose=self.verbose)        if self.app is None:            self.app = FastAPI()        log_http_service = LogHttpService()        self.add_http_interface(log_http_service, path='/log', module_name='日志查询服务')        for module_name, module in self.http_interfaces.items():            module.initialize_interface()            self.app.include_router(module)        for module_name, module in self.front_interfaces.items():            self.app = module.setup_app(self.app)    def add_front_interface(self, front_interface: FrontInterface, module_name: str, path: str = '/'):        """        Adds a front-end interface service to the current conversational application service.        This method integrates a front-end interface service into the conversational application        by associating it with a specific URL path and module name. It ensures that the path        is unique and starts with a slash ('/'). It also dynamically assigns the module name        and application name to the front-end interface.        Args:            front_interface (FrontInterface): The front-end interface service to be added.            module_name (str): The name of the module for the front-end interface.            path (str, optional): The URL path associated with the front-end interface. Defaults to '/'.        Raises:            ValueError: If the `path` does not start with '/'.            ValueError: If the `path` is already in use by another front-end interface.        """        # Validate the types of the provided arguments to ensure they match the expected types.        check_variable_type(front_interface, FrontInterface)        check_variable_type(module_name, str)        check_variable_type(path, str)        # Ensure the path starts with a '/' to maintain consistency and prevent routing errors.        if not path.startswith('/'):            raise ValueError("the parameter path must start with '/' as prefix")        # Check for duplicate paths to prevent conflicts between different front-end interfaces.        if module_name in self.front_interfaces:            raise ValueError(f"the module_name {module_name} is duplicated, please choose a different one.")        if path in self.paths:            raise ValueError(f"the path {path} is duplicated, please choose a different one.")        self.paths.append(path)        # Add the front-end interface to the dictionary of interfaces, using the path as the key.        self.front_interfaces[module_name] = front_interface        # Dynamically set the module_name and application_name properties on the front-end interface.        setattr(front_interface, 'module_name', module_name)        setattr(front_interface, 'application_name', self.application_name)        setattr(front_interface, 'path', path)        setattr(front_interface, 'chat_application', self)        setattr(front_interface, 'module_type', 'front')    def add_query_module(self,                         query_module: QueryModule,                         module_name: str,                         variable_name: str,                         query_field: str,                         top_k: int,                         result_fields: List[str],                         min_score: float = None,                         white_list: list[str] = None,                         black_list: list[str] = None):        """        Adds a knowledge database module to the current dialogue application.        Args:            black_list: Whitelist list, the module will only be executed when it is in the dialogue stage in the                        whitelist            white_list: Blacklist list, when in the dialogue stage in the blacklist, the module will not execute            query_module (QueryModule): The instance of the knowledge database module.            module_name (str): The unique name for the knowledge database module.            variable_name (str): The parameter name used in the inference method of the query database.            result_fields (List[str]): The list of fields to be returned in the query result.            query_field (str): The field to be matched in the database.            top_k (int): The number of knowledge entries to be returned by the query.            min_score (float, optional): The minimum similarity score for the returned knowledge. Defaults to None.        Raises:            ValueError: If the module_name is already present in the query_modules or think_modules.            TypeError: If the types of the inputs do not match the expected types.        """        if white_list is not None and black_list is not None:            raise ValueError("Blacklist and whitelist cannot be set at the same time.")        # Checks if the query_module is an instance of QueryModule.        check_variable_type(query_module, QueryModule)        # Checks if the module_name is a string.        check_variable_type(module_name, str)        # Raises an error if the module_name is already in use in query_modules.        if module_name in self.query_modules:            raise ValueError(f"The module_name {module_name} is duplicated in query_modules. Please change one.")        # Raises an error if the module_name is already in use in think_modules.        if module_name in self.think_modules:            raise ValueError(f"The module_name {module_name} is duplicated in think_modules. Please change one.")        # Adds the knowledge database module to the application with the given module_name.        self.query_modules[module_name] = {            'module': query_module,            'query_parameter_name': variable_name,            'query_field': query_field,            'top_k': top_k,            'result_fields': result_fields,            'min_score': min_score        }        if white_list is not None:            setattr(query_module, 'white_list', white_list)        if black_list is not None:            setattr(query_module, 'black_list', black_list)        if not hasattr(query_module, 'embedding_module'):            setattr(query_module, 'embedding_model', getattr(query_module, 'embedding_class')(                embedding_model_name=getattr(query_module, 'embed_type'),                openai_api_key=self.openai_api_key,                openai_api_base=getattr(query_module, 'openai_api_base'),                model=getattr(query_module, 'model'),                chunk_size=getattr(query_module, 'chunk_size'),                max_retries=getattr(query_module, 'max_retries')))    @convert_async_2_normal    async def query(self,                    query_module_names: List[str] = None,                    is_return_async_tasks: bool = False,                    **kwargs) -> Union[Dict[str, Any], List[Any]]:        """        Executes a query operation on the knowledge database. This method is asynchronous and is executed concurrently        during the inference and thinking stages of the dialogue application's lifecycle. The query results are then        processed by the process_query_results method.        Args:            query_module_names (List[str]): The names of the knowledge base query modules to be executed in the current            query operation. Defaults to None, which means all available modules will be used.            is_return_async_tasks (bool): Whether to only return a list of asynchronously executed tasks. Defaults to            False.        Returns:            Union[Dict[str, Any], List[Any]]: If is_return_async_tasks is True, returns a list of asynchronously            executed tasks. Otherwise, returns a dictionary with query module names as keys and their corresponding            query results as values.        Raises:            ValueError: If any of the specified query module names are not present in the query_modules.        """        # If no query module names are provided, use all available modules.        if query_module_names is None:            query_module_names = [module_name for module_name in self.query_modules.keys()]        # Check if all specified query module names are present in the query_modules.        query_module_names = [module_name for module_name in query_module_names if                              self.check_module_available(self.query_modules[module_name])]        for query_module_name in query_module_names:            if query_module_name not in self.query_modules:                raise ValueError(f"{query_module_name} not in the query_modules.")        # Get the query modules based on the specified names.        query_modules = [self.query_modules[query_module_name] for query_module_name in query_module_names]        # Create a list of asynchronous query tasks for each query module.        tasks = [query_module['module'].async_query(            query_content=kwargs.get(query_module['query_parameter_name'], ""),            query_field=query_module['query_field'],            top_k=query_module['top_k'],            result_fields=query_module['result_fields'],            min_score=query_module['min_score']        ) for query_module in query_modules]        # If is_return_async_tasks is True, return the list of tasks.        if is_return_async_tasks:            return tasks        # Otherwise, wait for all tasks to complete and gather their results.        results = await asyncio.gather(*tasks)        # Format the results into a dictionary with query module names as keys.        format_results = {}        for i, result in enumerate(results):            format_results[query_module_names[i]] = result        return format_results    @convert_async_2_normal    async def _query(self, is_return_async_tasks: bool = True, **query_module_inputs) -> Union[Dict, Tuple[List[str],                                                                                                           List[                                                                                                               asyncio.Task]]]:        """        Asynchronously executes queries on the knowledge database. This method is used during the inference and thinking        phase of a conversation application's lifecycle and runs concurrently.        Args:            is_return_async_tasks (bool): Whether to return a list of asynchronous tasks instead of the actual query            results.            query_module_inputs (dict): Input parameters for the knowledge base query modules.        Returns:            Union[Dict[str, List[Dict]], Tuple[List[str], List[asyncio.Task]]]:            If is_return_async_tasks is True, returns a tuple containing the names of the query modules and a list of            their corresponding asyncio tasks.            Otherwise, returns a dictionary with query module names as keys and their corresponding query results as            values.        """        # Get the names of all query modules        query_module_names = [module_name for module_name in self.query_modules.keys() if                              self.check_module_available(self.query_modules[module_name])]        # Get the query modules themselves        query_modules = [self.query_modules[query_module_name] for query_module_name in query_module_names]        # Create a list of asyncio tasks for each query module        tasks = [query_module['module'].async_query(            query_content=get_dict_value(query_module_inputs,                                         [query_module_names[i],                                          query_module['query_parameter_name']]),            query_field=query_module['query_field'],            top_k=query_module['top_k'],            result_fields=query_module['result_fields'],            min_score=query_module['min_score']        ) for i, query_module in enumerate(query_modules)]        # If is_return_async_tasks is True, return the query module names and their corresponding tasks        if is_return_async_tasks:            return query_module_names, tasks        # If is_return_async_tasks is False, wait for all tasks to complete and gather their results        results = await asyncio.gather(*tasks)        # Format the results into a dictionary with query module names as keys        format_results = {}        for i, result in enumerate(results):            format_results[query_module_names[i]] = result        return format_results    @convert_async_2_normal    @add_call_async_func_2_log    async def _think_and_query(self, **think_and_query_inputs):        """        Concurrently executes thinking and reasoning operations along with knowledge base queries.        The results are then passed back to the process_think_results and process_query_results methods,        before exiting the decorator. Finally, the update_chat_stage method is called.        Args:            think_module_names: A list of names of modules to use for thinking.            query_module_names: A list of names of modules to use for querying the knowledge database.            **kwargs: Additional keyword arguments to be passed to the think and query operations.        Returns:            A tuple containing two dictionaries: one with the thinking results and one with the query results.        """        # Perform thinking operations asynchronously and return the tasks.        think_module_names, think_tasks = await self._think(is_return_async_tasks=True,                                                            **think_and_query_inputs)        # Perform knowledge base queries asynchronously and return the tasks.        query_module_names, query_tasks = await self._query(is_return_async_tasks=True,                                                            **think_and_query_inputs)        # Combine the thinking and querying tasks.        tasks = think_tasks + query_tasks        # Wait for all tasks to complete and gather the results.        results = await asyncio.gather(*tasks)        # Split the results into thinking and querying results based on the number of modules.        think_results_ = results[:len(think_tasks)]        query_results_ = results[len(think_tasks):]        # Create a dictionary mapping module names to thinking results.        think_results = {think_module_names[i]: think_result for i, think_result in enumerate(think_results_)}        # Create a dictionary mapping module names to query results.        query_results = {query_module_names[i]: query_result for i, query_result in                         enumerate(query_results_)}        # Return the dictionaries containing the thinking and query results.        return think_results, query_results    @abstractmethod    def update_original_chat_stage(self, inference_inputs: Dict) -> str:        """        Determines the original dialogue stage at the time of the request, which is crucial for the algorithm.        This method requires you to analyze the sent request to identify the original dialogue stage and return it.        Args:            inference_inputs: The results of the inference performed on the request, which should contain                information to deduce the original chat stage.        Returns:            The identified original chat stage.        Raises:            NotImplementedError: This method must be implemented by subclasses.        """        pass  # Subclasses should override this method and provide an actual implementation.    @abstractmethod    def update_store(self,                     store: Store,                     think_results: Dict,                     query_results: Dict,                     original_chat_stage: str,                     inference_inputs: Dict) -> Store:        """        Processes the results from the reasoning and knowledge query modules. It is recommended to handle the user's        information data and define it within an object of the Store class. This method must return an object of the        Store class.        Args:            original_chat_stage: The original chat stage before updating.            store: A global storage object that holds the state of the system.            think_results: The outcomes of the reasoning process.            query_results: The outcomes of the knowledge query process.            inference_inputs: The input parameters that were used to access the inference method.        Returns:            store: An updated Store object that includes the processed information from the think_results,            query_results, and inference_inputs.        Raises:            NotImplementedError: This method must be implemented by subclasses.        """        pass  # Subclasses should override this method and provide an actual implementation.    @abstractmethod    def gen_reply_inputs(self,                         current_chat_stage: str,                         store: Store,                         think_results: Dict,                         query_results: Any,                         original_chat_stage: str,                         inference_inputs: Dict) -> Dict:        """        Determines how to generate appropriate placeholder variables for the current chat module based on the        identified dialogue stage. The result returned by this method will be used as the input parameter for        executing the `reply` method.        Args:            original_chat_stage: The chat stage before update.            store: The global storage object containing the state of the system.            current_chat_stage: A string representing the current dialogue stage.            think_results: A dictionary of results from the thinking process.            query_results: The outcomes from the knowledge query process, which can be of any type.            inference_inputs: A dictionary of inputs that were used for inference.        Returns:            A dictionary of inputs to be used by the chat module, tailored based on the current dialogue stage and            provided results.        Raises:            NotImplementedError: This method must be implemented by subclasses.        """        pass  # Subclasses should override this method and provide an actual implementation.    @convert_async_2_normal    @add_call_async_func_2_log    async def evaluate(self,                       is_return_async_tasks=False,                       evaluate_module_names: list = None,                       **kwargs) -> Union[Dict, Tuple[list, list]]:        """        Evaluates the results using the provided inputs for each module.        Args:            is_return_async_tasks: A boolean indicating whether to return the asynchronous tasks.            evaluate_module_names: A list of names of the evaluation modules to be executed. If None, all modules will            be executed.            kwargs: the inputs of all evaluate modules.        Returns:            A dictionary of formatted results or a tuple of lists containing module names and asynchronous tasks,            depending on the value of is_return_async_tasks.        """        # If no specific module names are provided, use all available module names.        if evaluate_module_names is None:            evaluate_module_names = [module_name for module_name in self.evaluate_modules.keys()]        evaluate_module_names = [module_name for module_name in evaluate_module_names if                                 self.check_module_available(self.evaluate_modules[module_name])]        # Verify that all specified module names exist.        for evaluate_module_name in evaluate_module_names:            if evaluate_module_name not in self.evaluate_modules:                raise ValueError(f"{evaluate_module_name} does not exist!")        # Get the evaluation modules that are to be executed.        evaluate_modules = [self.evaluate_modules[module_name] for module_name in evaluate_module_names]        # Create a list of asynchronous evaluation tasks for each module.        async_task = [            evaluate_module.async_evaluate(                **get_dict_values(kwargs, [evaluate_module_names[i]], evaluate_module.input_variables)            )            for i, evaluate_module in enumerate(evaluate_modules)        ]        # If requested, return the module names and the async tasks.        if is_return_async_tasks:            return evaluate_module_names, async_task        # Wait for all asynchronous tasks to complete and gather the results.        results = await asyncio.gather(*async_task)        # Format the results into a dictionary with module names as keys.        format_results = {evaluate_module_names[i]: results[i] for i, evaluate_module in enumerate(evaluate_modules)}        # Return the formatted results.        return format_results    @convert_async_2_normal    @add_call_async_func_2_log    async def _evaluate(self,                        is_return_async_tasks=True,                        **kwargs) -> Union[Dict, Tuple[list, list]]:        """        Evaluates the results using the provided inputs for each module.        Args:            is_return_async_tasks: A boolean indicating whether to return the asynchronous tasks.            kwargs: the inputs of all evaluate modules.        Returns:            A dictionary of formatted results or a tuple of lists containing module names and asynchronous tasks,            depending on the value of is_return_async_tasks.        """        # If no specific module names are provided, use all available module names.        evaluate_module_names = [module_name for module_name in self.evaluate_modules.keys() if                                 self.check_module_available(self.evaluate_modules[module_name])]        # Verify that all specified module names exist.        for evaluate_module_name in evaluate_module_names:            if evaluate_module_name not in self.evaluate_modules:                raise ValueError(f"{evaluate_module_name} does not exist!")        # Get the evaluation modules that are to be executed.        evaluate_modules = [self.evaluate_modules[module_name] for module_name in evaluate_module_names]        # Create a list of asynchronous evaluation tasks for each module.        async_task = [            evaluate_module.async_evaluate(                **get_dict_values(kwargs, [evaluate_module_names[i]], evaluate_module.input_variables)            )            for i, evaluate_module in enumerate(evaluate_modules)        ]        # If requested, return the module names and the async tasks.        if is_return_async_tasks:            return evaluate_module_names, async_task        # Wait for all asynchronous tasks to complete and gather the results.        results = await asyncio.gather(*async_task)        # Format the results into a dictionary with module names as keys.        format_results = {evaluate_module_names[i]: results[i] for i, evaluate_module in enumerate(evaluate_modules)}        # Return the formatted results.        return format_results    @abstractmethod    def gen_think_and_query_inputs(self, store: Store, inference_inputs: Dict) -> Dict:        """        Generates the actual values or parameter values for placeholders required by the thinking inference module and        the knowledge query module.        Args:            store: The storage of key values and attributes.            inference_inputs: The parameters passed to the inference method.        Returns:            Dict: A dictionary containing the actual values or parameter values for the placeholders.        This method should be implemented by subclasses to provide the necessary inputs for the thinking and querying        processes.        """        # This is an abstract method, so it only provides the method signature and documentation.        # Subclasses must override this method to provide the actual implementation.        pass    @abstractmethod    def request_preprocess(self, request_inputs: Dict) -> Callable:        """        This method is triggered when a request is sent to the current application. It is responsible for preprocessing        the request before specifying which method should handle it.        Args:            request_inputs (Dict): A dictionary containing the inputs of the request. This should include all necessary                data required to process the request.        Returns:            None: This method does not return anything. It should modify the request inputs as needed or prepare the            environment for the subsequent request handling method.        """        # Your preprocessing logic here        pass    def add_http_interface(self, http_interface: HttpInterface, path: str, module_name: str = None):        """        Adds an HTTP interface module to the chat application.        This method incorporates an HTTP interface module into the chat application,        assigning it a URL path and an optional module name for identification.        Args:            http_interface (HttpInterface): The HTTP interface module to be added.            path (str): The URL path for the HTTP service interface. It must be a string                that starts with a forward slash ('/').            module_name (str, optional): The name identifier for the HTTP interface module.                If not provided, the name of the class file will be used by default.        Returns:            None.        Raises:            ValueError: If the provided `url_path` does not start with a forward slash ('/').        """        # Check that the http_interface is an instance of HttpInterface.        check_variable_type(http_interface, HttpInterface)        # Check that the path is a string.        check_variable_type(path, str)        # Ensure the URL path starts with a forward slash.        if not path.startswith('/'):            raise ValueError(f"{path} must start with '/'.")        # If no module name is provided, use the name of the class file.        if module_name is None:            module_name = inspect.getfile(http_interface.__class__)        if path in self.paths:            raise ValueError(f"the path {path} is duplicated, please choose a different one.")        self.paths.append(path)        # Store the http_interface with its module_name in the dictionary.        self.http_interfaces[module_name] = http_interface        # Set attributes on the http_interface object for later reference.        setattr(http_interface, 'chat_application', self)        setattr(http_interface, 'module_name', module_name)        setattr(http_interface, 'application_name', self.application_name)        setattr(http_interface, 'path', path)    def check_module_available(self, module) -> bool:        """        Check if the given module is available for execution in the current stage.        This function determines the availability of a module based on its white list and black list attributes.        A module is considered available if:        - It has neither a white list nor a black list.        - It has a white list, and the current stage is in the white list.        - It has a black list, and the current stage is not in the black list.        A module is considered unavailable if it has both a white list and a black list.        Args:            module: The module object to check. This object is expected to potentially have 'white_list' and            'black_list' attributes, which should be lists of stages where the module is explicitly allowed or            disallowed.        Returns:            bool: True if the module is available for execution in the current stage, False otherwise.        """        if not hasattr(module, 'white_list') and not hasattr(module, 'black_list'):            # The module has neither a white list nor a black list, so it's considered available.            return True        elif hasattr(module, 'white_list') and hasattr(module, 'black_list'):            # The module has both a white list and a black list, which is a conflicting configuration, so it's            # considered unavailable.            return False        elif hasattr(module, 'white_list') and self.current_stage in getattr(module, 'white_list'):            # The module has a white list and the current stage is in the white list, so it's considered available.            return True        elif hasattr(module, 'black_list') and self.current_stage not in getattr(module, 'black_list'):            # The module has a black list and the current stage is not in the black list, so it's considered available.            return True        else:            # If none of the above conditions are met, the module is considered unavailable.            return False    @abstractmethod    def set_store_class(self) -> type:        """        Sets the class type for the storage object 'store'.        This method is intended to be implemented by subclasses, where it should        return the class type of the specific storage object they wish to use.        For example, if your storage class is `MyStorageClass`, you would implement        this method to simply `return MyStorageClass`.        Returns:            type: The class type of the storage object.        """        pass    @abstractmethod    def load_store(self, inference_inputs: Dict, store: Store) -> Store:        """        Reloads the `store` object based on the request data received for the first time.        This abstract method is intended to be implemented by subclasses to define how the `store` object should        be reloaded or updated based on the `inference_inputs` provided. This is typically used in scenarios where        the initial request data needs to be processed to update or modify the state of the `store` object before        performing further operations.        Args:            inference_inputs: A dictionary containing the parameters for the inference method call. This dict                              provides the necessary information required to update the `store`.            store: An instance of a `Store` object that needs to be reloaded or updated. This object represents                   a storage mechanism or a state that is being maintained across different operations or                   method calls.        Returns:            A `Store` object that has been reloaded or updated based on the `inference_inputs`. This updated            store object should reflect any changes or modifications made as a result of processing the            `inference_inputs`.        """        pass