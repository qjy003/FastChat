import reimport copyfrom typing import Union, Listfrom langchain.llms.base import LLMfrom langchain_core.callbacks import CallbackManagerForLLMRunfrom langchain_core.language_models import LanguageModelInputfrom langchain_core.outputs import GenerationChunkfrom langchain_core.pydantic_v1 import BaseModelfrom langchain_core.runnables import Runnablefrom langchain_core.runnables.utils import Input, Outputfrom langchain_core.runnables.config import RunnableConfigfrom pydantic.v1.main import ModelMetaclassfrom src.base.utils import return_type_must_be, split_string_at_last_caretfrom src.base.utils import add_call_func_2_log, trans_prompt_formatfrom abc import abstractmethod, ABCMetafrom typing import Optional, List, Any, Iterator, Union, Dict, Typeclass _DecorateAllMethods(type):    """A metaclass that decorates all methods of a subclass.    This metaclass iterates over all attributes of a class being created. If an attribute is a callable    (i.e., a method), it checks if the method is overriding a method from its base classes. If it is,    and if the method name matches specific criteria, it applies a decorator to the method. This approach    ensures that methods overriding abstract methods still retain the intended decorators from the superclass.    """    def __new__(mcs, name, bases, dct):        # Iterate over all attributes of the class        for attr, value in dct.items():            # Check if the attribute is a callable (i.e., likely a method)            if callable(value):                # Check if this method overrides a method from its base classes                for base in bases:                    if hasattr(base, attr) and getattr(base, attr) is not value:                        break                # Apply specific decorators based on the method's name and whether it overrides a base class method                if attr == 'inference':                    dct[attr] = return_type_must_be(str)(trans_prompt_format(value))                if attr == "my_inference":                    dct[attr] = return_type_must_be(str)(add_call_func_2_log(value))        # Create the new class with the possibly modified dictionary of attributes/methods        return super().__new__(mcs, name, bases, dct)class _CombinedMeta(ModelMetaclass, _DecorateAllMethods):    """    In order to avoid conflicts caused by other classes inherited by the class that are    not of the same origin as the primitive class, the primitive class and abstract class    ABCMeta are merged to avoid conflicts.    """    passclass PrivateModel(LLM, metaclass=_CombinedMeta):    """    For a privately deployed large model, the reasoning process implemented by the model needs to be implemented by the     R&D personnel themselves, usually by accessing the open source large model through the HTTP service interface built     by a specific backend framework.    """    @abstractmethod    def inference(self, request_id: str, prompt: Union[str, list]) -> str:        """        This is an abstract method that must be implemented by developers.        It is responsible for obtaining the inference result from the large model        based on the provided prompt.        Args:            request_id (str): HTTP service request ID.            prompt (str): The prompt for the large model.        Returns:            str: The inference result obtained from the large model.        Note:            - This method does not provide an implementation and must be overridden              in a subclass.            - The purpose of this method is to allow for flexible implementations              depending on the specific large model being used.        """        pass    def my_inference(self, request_id: str, prompt: Union[str, list]) -> str:        """        PrivateModel类内置推理方法，前置处理用户+系统构建的prompt，提取系统编写内容。目前主要提取request_id信息完成日志记录。        2025-08-22        Args:            request_id: http服务请求ID            prompt: 构造好的提示词，由用户填写内容及系统补充共同构成        Returns:            大模型推理的结果        """        result = self.inference(request_id, prompt)        return result    @property    def _llm_type(self) -> str:        return "private_model"    def _call(            self,            prompt: str,            stop: Optional[List[str]] = None,            run_manager: Optional[CallbackManagerForLLMRun] = None,            **kwargs: Any,    ) -> str:        """        The lowest level of the large model class built into langchain is a method for obtaining large model inference        results based on the generated prompt word content. After the interface method is developed and implemented,        this method will be called in multiple locations.        Args:            prompt: The prompt to generate from.            stop: Stop words to use when generating. Model output is cut off at the                first occurrence of any of the stop substrings.                If stop tokens are not supported consider raising NotImplementedError.            run_manager: Callback manager for the run.            **kwargs: Arbitrary additional keyword arguments. These are usually passed                to the model provider API call.        Returns:            The model output as a string. SHOULD NOT include the prompt.        """        user_prompt, request_id = split_string_at_last_caret(prompt)        result = self.my_inference(request_id=request_id, prompt=user_prompt)        return result    def _stream(            self,            prompt: str,            stop: Optional[List[str]] = None,            run_manager: Optional[CallbackManagerForLLMRun] = None,            **kwargs: Any,    ) -> Iterator[GenerationChunk]:        """        Use streaming to output the results generated by large model reasoning one by one        Args:            prompt: The prompt to generate from.            stop: Stop words to use when generating. Model output is cut off at the                first occurrence of any of these substrings.            run_manager: Callback manager for the run.            **kwargs: Arbitrary additional keyword arguments. These are usually passed                to the model provider API call.        Returns:            An iterator of GenerationChunks.        """        try:            result = self.inference(prompt)            yield GenerationChunk(text=result,                                  generation_info={"input_tokens": len(prompt),                                                   "output_tokens": len(result)}                                  )        except Exception as e:            error_info = f"响应失败，请重新检查，失败信息为{e}"            yield GenerationChunk(text=error_info,                                  generation_info={"input_tokens": len(prompt),                                                   "output_tokens": len(error_info)})    def with_structured_output(            self, schema: str, **kwargs: Any    ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:        """        Creates a runnable object that accepts either a dictionary or a BaseModel object as input,        and produces an output based on the large model's inference using that input.        This function allows for structured inputs to be dynamically integrated into a template        and then passed to the large model for inference, enabling more flexible and context-aware        applications of the model.        Args:            schema: A template string that outlines the basic structure of the input to the model.                   This template can include placeholders that will be filled with actual values                   from the input dictionary or BaseModel object.            **kwargs: Additional keyword arguments that can be passed into the model. These are not                      used directly in this function but can be useful for extending its functionality.        Returns:            A Runnable object that can be invoked with a structured input to obtain the large model's            inference results. This object encapsulates the logic for preparing the input, performing            the inference, and returning the result.        """        def extract_braced_variables(s: str) -> set:            """            Identifies and extracts variables enclosed in curly braces within a string.            This utility function is used to parse the schema template and identify placeholders            that need to be filled with actual values when constructing the input to the model.            Args:                s: The string to parse, typically the schema template.            Returns:                A set of variable names (as strings) that were enclosed in curly braces in the input string.            """            # Use regular expression to find all occurrences of text within curly braces            matches = re.findall(r'\{([^{}]+)\}', s)            # Convert the list of matches to a set to remove duplicates            variables = set(matches)            return variables        template = schema        input_variables = extract_braced_variables(template)        class MyRunnable(Runnable):            """            A specialized Runnable that takes structured input and produces output based on            the large model's inference. It is tailored to work with the specified template            and expected input variables.            """            def __init__(self):                self.input_variables = input_variables                self.template = template            def invoke(_self, input: Dict, config: Optional[RunnableConfig] = None) -> Output:                """                Accepts a dictionary as input, fills the placeholders in the template with                values from this dictionary, and then performs model inference with the                resulting prompt.                This method dynamically constructs the input prompt for the model based on                the structured input and the predefined template, allowing for flexible and                context-specific inferences.                Args:                    input: A dictionary containing the structured data to be used as input.                           This data is used to fill in the placeholders in the template.                    config: Optional configuration for the inference run. This can include                            settings like the timeout, the model to use, etc.                Returns:                    The result of the model inference as an Output object. The exact nature                    of this output can vary depending on the model and the input.                """                prompt = copy.deepcopy(_self.template)                # Ensure the input is a dictionary; raise an error otherwise                if not isinstance(input, Dict):                    raise ValueError("The input variable must be Dict type value.")                # Convert BaseModel input to dictionary if necessary                input = input.model_dump() if isinstance(input, BaseModel) else input                # Check if all required variables are present in the input                keys = set(input.keys())                if not _self.input_variables.issubset(keys):                    missing_variables = list(input_variables.difference(keys))                    missing_variables_str = ",".join(missing_variables)                    raise ValueError(f"The input misses the variables {missing_variables_str}")                # Replace placeholders in the template with actual values from the input                for key, value in input.items():                    prompt = prompt.replace("{" + f"{key}" + "}", value)                # Perform inference with the constructed prompt and return the result                result = self.inference(prompt=prompt)                return result        return MyRunnable()