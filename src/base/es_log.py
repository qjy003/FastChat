import asyncioimport base64import gzipimport loggingimport jsonimport pickleimport requestsimport osimport jsonfrom src.config import *from elasticsearch import Elasticsearchfrom datetime import datefrom datetime import datetimefrom datetime import timedeltafrom src.config import LOGGING_DIRfrom typing import Union, Anylogging.basicConfig()def parse_datetime(date_str):    """    尝试将给定字符串解析为datetime对象。    参数:    date_str: str - 需要解析的日期时间字符串。    返回:    datetime - 成功解析后的datetime对象。    异常:    ValueError - 如果所有支持的格式都无法解析date_str，抛出此异常。    格式支持:    该函数支持多种日期时间格式，包括微秒和毫秒表示。    格式列表如下：    - "%Y-%m-%d %H:%M:%S,%f"    - "%Y-%m-%d %H:%M:%S.%f"    - "%Y-%m-%d %H:%M:%S"    - "%Y-%m-%d %H:%M"    - "%Y-%m-%d %H"    - "%Y-%m-%d"    """    # 遍历支持的日期时间格式列表尝试解析    for fmt in (            "%Y-%m-%d %H:%M:%S,%f", "%Y-%m-%d %H:%M:%S.%f", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%Y-%m-%d %H",            "%Y-%m-%d"):        try:            return datetime.strptime(date_str, fmt)  # 尝试使用当前格式解析字符串        except ValueError:            continue  # 如果解析失败，尝试下一个格式    raise ValueError("未知的日期时间格式")  # 如果所有格式都尝试过且失败，抛出异常def check_log_contains(log_str: str, query_contents: Union[list, set, str]) -> bool:    """    Checks if the log string contains the specified content.    Args:        log_str (str): The log data to search within.        query_contents (Union[list, set, str]): The content to search for in the log data.            If a list, all elements must be found for the function to return True.            If a set, at least one element must be found for the function to return True.            If a string, the string must be found for the function to return True.    Returns:        bool: True if the log data contains the specified content, False otherwise.    """    # If the query_contents is a list, check if all elements are in the log_str    if isinstance(query_contents, list):        return all(check_log_contains(log_str, query_content) for query_content in query_contents)    # If the query_contents is a set, check if any element is in the log_str    elif isinstance(query_contents, set):        return any(check_log_contains(log_str, query_content) for query_content in query_contents)    # If the query_contents is a string, check if it is in the log_str    else:        return query_contents in log_strclass JsonFormatter(logging.Formatter):    """    JsonFormatter stores log data in JSON format, facilitating the use of log data for algorithm debugging.    This class subclasses the standard logging.Formatter to output log records in JSON format.    It is particularly useful when log data needs to be parsed or analyzed by machine-readable means.    """    def format(self, record):        """        Format the log record into a JSON string.        This method creates a dictionary with the log record's data, including a formatted timestamp,        and then serializes it to a JSON string. The 'ensure_ascii=False' parameter allows for the        inclusion of non-ASCII characters in the JSON output.        Args:            record (logging.LogRecord): The log record to be formatted.        Returns:            str: A JSON-formatted string representing the log record.        """        # Create a log record dictionary with a formatted timestamp        log_record = {'timestamp': self.formatTime(record)}        # Update the log record dictionary with the rest of the record's attributes        log_record.update(vars(record))        # Serialize the log record dictionary to a JSON string, ensuring non-ASCII characters are preserved        return json.dumps(log_record, ensure_ascii=False)class Logger:    """    Logger class provides logging services with the following capabilities:        1. Writes data into log files within a container.        2. Searches for log data within the container that matches filtering criteria.        3. Retrieves log data from within the container via HTTP requests locally.    This class is designed to manage logging for an application, providing a centralized    location for log data storage and access.    """    def __init__(self, es_table_name=Elasticsearch_Table, elasticsearch_params=Elasticsearch_Params):        """        Initialize the Logger instance.        Sets up the logger with a file handler that writes to a log file named after the current date.        The log folder path is expected to be defined in a constant `LOGGING_DIR`.        Args:            es_table_name (str): The name of the ES Table for saving log data            elasticsearch_params (dict): The host of the ES Server        """        self.es = Elasticsearch(**elasticsearch_params)        self.es_table_name = es_table_name    def write(self, status: str, **kwargs):        """        Writes data to be logged into the log file.        This method logs information based on the provided status. If the status is 'fail',        it logs a warning with the current time, exception information, stack information,        and any additional keyword arguments. Otherwise, it logs a debug message with the        same details.        Args:            status (str): The status of the log entry ('fail' for warnings, any other value for debug).            **kwargs (dict): Additional keyword arguments to be included in the log entry.        Returns:            None        """        kwargs = {k: json.dumps(v, ensure_ascii=False) if isinstance(v, dict) else ("" if v is None else str(v)) for                  k, v in kwargs.items()}        if 'composed_prompt' not in kwargs:            kwargs['composed_prompt'] = ""        start_time = datetime.strftime(parse_datetime(kwargs.get('start_timestamp')), "%Y-%m-%dT%H:%M:%S,%f")        end_time = datetime.strftime(parse_datetime(kwargs.get('end_timestamp')), "%Y-%m-%dT%H:%M:%S,%f")        kwargs['start_timestamp'] = start_time        kwargs['end_timestamp'] = end_time        self.es.index(index=self.es_table_name, doc_type="sources", body=kwargs)    def response_search_log(self, **kwargs):        """        Searches for log entries that match the given filter criteria and returns the        qualifying log data information.        The method iterates through the logs within the specified time range, checking        each line against the provided keyword arguments for matches. Matching log        entries are added to the search results.        Args:            time_zone (tuple): A tuple containing the start and end dates for the search.            query_contents (list[str]): the query_contents        Returns:            dict: A dictionary containing the compressed and base64-encoded search results.        """        time_zone = kwargs.get('time_zone')        query_contents = kwargs.get('query_contents')[0] if len(kwargs.get('query_contents')) > 0 else ""        start_date = datetime.strftime(parse_datetime(time_zone[0]), '%Y-%m-%dT%H:%M:%S,%f')        end_date = datetime.strftime(parse_datetime(time_zone[1]), '%Y-%m-%dT%H:%M:%S,%f')        search_results = {}        must = [{            "range": {                "start_timestamp": {                    "gte": start_date,                    "lte": end_date                }            }        }]        query = {            "query": {                "bool": {                    "must": must                }            },            "size": 1000        }        if query_contents != "":            query['query']['bool']["should"] = [                {"match_phrase": {                    "inputs": query_contents                }},                {"match_phrase": {                    "result": query_contents                }}            ]            query['query']['bool']['minimum_should_match'] = 1        response = self.es.search(index=self.es_table_name, body=query)        hits = response['hits']['hits']        for i, hit in enumerate(hits):            try:                hit['_source']['inputs'] = json.loads(hit['_source']['inputs'])            except:                pass            try:                hit['_source']['result'] = json.loads(hit['_source']['result'])            except:                pass            search_results[i] = hit['_source']        # Serialize the search results using pickle, compress with gzip, and encode with base64        # pres = pickle.dumps(search_results)        # gzip_res = gzip.compress(pres)        # return {"data": base64.b32encode(gzip_res), 'fast_chat_code': 'es_log', 'num': len(search_results)}        return {"data": search_results, 'fast_chat_code': 'es_log', 'num': len(search_results)}    def create_table(self):        """        Create ES Table on ES server        """        if not self.es.indices.exists(index=self.es_table_name):            try:                table_body = {                    "settings": {                        "number_of_shards": 3,                        "number_of_replicas": 0                    },                    "mappings": {                        "sources": {                            "properties": {  # 字段名及字段属性类型的定义                                "spent_time": {"type": "text"},                                "start_timestamp": {"type": "date"},                                "end_timestamp": {"type": "date"},                                "inputs": {"type": "text"},                                "application_name": {"type": "text"},                                "method_name": {"type": "text"},                                "module_name": {"type": "text"},                                "class_name": {"type": "text"},                                "module_type": {"type": "text"},                                "result": {"type": "text"},                                "composed_prompt": {"type": "text"}                            }                        }                    }                }                self.es.indices.create(index=self.es_table_name, body=table_body)            except Exception as e:                print(e)                return False            return True        else:            return True    @staticmethod    async def _request_log(url: str, time_zone: tuple, query_contents: Union[list, set, str]):        """        Retrieves log data from a specified URL.        This method constructs a request to the provided URL with the intention of        fetching log data within the specified time zone. It ensures the URL has the        correct format and merges additional keyword arguments into the request body.        Args:            url (str): The network address and interface for the log request.            time_zone (tuple): A tuple indicating the time interval for the log data.            **kwargs (dict): Additional keyword arguments to include in the request body.        Returns:            requests.Response: The Response object from the HTTP request.        """        # Ensure the URL starts with 'http://' if not already present        if not url.startswith('http://'):            url = 'http://' + url        # Ensure the URL ends with '/log' if not already present        if not url.endswith('/log'):            url = url + '/log'        # Print the final URL for debugging purposes        print(url)        # Construct the request data by merging time_zone and any additional kwargs        request_data = {'time_zone': time_zone,                        'query_contents': query_contents}        # Note: This line will not work in an async function. For synchronous use only.        res = requests.post(url, json=request_data)        # Return the Response object        return res    async def request_log(self, urls: list, time_zone: tuple, query_contents: Union[list, set, str],                          log_folder_path: str = 'data/json/'):        """        Concurrently retrieves log data from multiple URLs and aggregates the results.        This method initiates asynchronous requests to retrieve log data from the provided        list of URLs within the specified time zone. It then decodes, decompresses, and        deserializes the responses to aggregate the log data. The aggregated data is        saved to a JSON file in the specified log folder path.        Args:            query_contents: the content we hope exists in the log has            urls (list): A list of URLs from which to fetch log data.            time_zone (tuple): A tuple indicating the time interval for the log data.            log_folder_path (str): The path where the log files are stored.                Default is 'data/json/'.        Returns:            None. The aggregated log data is saved to a file on disk.        """        # List to hold tasks for asynchronous log requests        request_log_tasks = []        # Create a task for each URL to be requested asynchronously        for url in urls:            request_log_tasks.append(                asyncio.create_task(                    self._request_log(                        url=url,                        time_zone=time_zone,                        query_contents=query_contents                    )                )            )        # Wait for all the tasks to complete and gather the results        res_list = await asyncio.gather(*request_log_tasks)        # Aggregate the results into a single dictionary        all_res = {}        for res in res_list:            if res.status_code != 200:                raise ValueError(f"query fails by the code {res.status_code}.")            # Assuming the response is JSON with base64-encoded, gzipped pickle data            d = res.json()            try:                gzip_data = base64.b32decode(d['data'])                pkl_data = gzip.decompress(gzip_data)                bad_cases = pickle.loads(pkl_data)                all_res.update(bad_cases)            except:                pass        # Construct the path for the merged log file        path = os.path.join(log_folder_path, f"merged-data-{datetime.now().strftime('%Y-%m-%d%H%M%S-%f')}.json")        # Output message based on whether log data was found        if len(all_res) > 0:            print(f'Found {len(all_res)} log entries that meet the criteria, saved to {path}')        else:            print('No log entries meeting the criteria were found!')        # Write the aggregated log data to a JSON file        with open(path, "w", encoding="utf-8") as f:            f.write(json.dumps(all_res, ensure_ascii=False))