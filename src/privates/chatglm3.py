import jsonimport timeimport requestsfrom src.base.private import PrivateModelclass ChatGLM3(PrivateModel):    def inference(self, prompt: str) -> str:        """        大模型推理生成执行的结果        Args:            prompt: 大模型推理所需要依据的提示词        Returns: 大模型推理生成的结果        """        try:            # 计算提示文本的长度            current_length = len(prompt)            # 构造请求的数据，包括输入文本和参数            data = {                "text_input": prompt,                "parameters": {                    "stream": True,                    "temperature": 0,                    "max_tokens": 512                }            }            # 记录请求开始的时间            start_time = time.time()            # 发送POST请求            url = "http://inference-internal.yanxi-cap.jd.com/inference/service-81kcvavl1x/v1/chat/completions"            response = requests.post(url, data=json.dumps(data), stream=True)            # 检查HTTP响应状态码            if response.status_code == 200:                result = ""                # 读取响应内容的每个块，并进行处理                for chunk in response.iter_content(chunk_size=None, decode_unicode=True):                    # 解析JSON格式的响应内容                    output_json = json.loads(chunk[5:])                    # 累加新接收的文本输出                    result += output_json["text_output"][current_length:]                    # 更新当前输出文本的长度                    current_length = len(output_json["text_output"])                # 返回累加的结果                return result            else:                # 如果响应状态码不是200，打印错误信息                print(f"请求失败，状态码：{response.status_code}, {response.text}")                return ""        except requests.exceptions.RequestException as e:            # 打印请求异常            print(f"请求异常：{e}")            return ""